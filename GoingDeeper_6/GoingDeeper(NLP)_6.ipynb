{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYDvdIX0AtRc"
      },
      "source": [
        "# **프로젝트: 멋진 챗봇 만들기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ecLUvBWAi5l"
      },
      "source": [
        "# **Step1. Import Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjT7D01WA7uf"
      },
      "outputs": [],
      "source": [
        "# Mecab 설치\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy JPype1-py3\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKOoHS7IAiWr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from konlpy.tag import Mecab\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from tqdm import tqdm_notebook\n",
        "import gensim\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvaPDeayADad"
      },
      "source": [
        "# **Step2. Import Data**\n",
        "데이터는 https://github.com/songys/Chatbot_data 에서 ChatbotData .csv파일을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "m7DZFz_WAIr-",
        "outputId": "3908736f-f621-4e7d-886e-0c3eba9866e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11823, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66a5510d-9934-49f9-93b2-42e87129dfde\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66a5510d-9934-49f9-93b2-42e87129dfde')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-66a5510d-9934-49f9-93b2-42e87129dfde button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-66a5510d-9934-49f9-93b2-42e87129dfde');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/GoingDeeper_Data/GD6/ChatbotData.csv')\n",
        "print(data.shape)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW3DX6KeBwHn",
        "outputId": "927ea3c6-7319-4a99-8d63-091119eba618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q : ['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다']\n",
            "A : ['하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.']\n"
          ]
        }
      ],
      "source": [
        "questions = list(data.Q)\n",
        "answers = list(data.A)\n",
        "print('Q :',questions[:3])\n",
        "print('A :',answers[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvJmcaFjHr_7"
      },
      "source": [
        "문장길이에 대한 분포를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "id": "3GutPrIKIH5H",
        "outputId": "936bd7dc-1e5f-44a8-e98a-c3985bd6e061"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.200e+01, 7.300e+01, 4.100e+02, 9.200e+02, 1.648e+03, 1.673e+03,\n",
              "        1.743e+03, 7.320e+02, 1.318e+03, 9.800e+02, 7.110e+02, 5.070e+02,\n",
              "        3.340e+02, 1.950e+02, 1.330e+02, 7.700e+01, 1.080e+02, 6.100e+01,\n",
              "        5.300e+01, 3.000e+01, 1.600e+01, 2.100e+01, 1.700e+01, 9.000e+00,\n",
              "        7.000e+00, 8.000e+00, 7.000e+00, 5.000e+00, 3.000e+00, 4.000e+00,\n",
              "        3.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 2.000e+00, 0.000e+00,\n",
              "        2.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n",
              " array([ 1.   ,  2.875,  4.75 ,  6.625,  8.5  , 10.375, 12.25 , 14.125,\n",
              "        16.   , 17.875, 19.75 , 21.625, 23.5  , 25.375, 27.25 , 29.125,\n",
              "        31.   , 32.875, 34.75 , 36.625, 38.5  , 40.375, 42.25 , 44.125,\n",
              "        46.   , 47.875, 49.75 , 51.625, 53.5  , 55.375, 57.25 , 59.125,\n",
              "        61.   , 62.875, 64.75 , 66.625, 68.5  , 70.375, 72.25 , 74.125,\n",
              "        76.   ]),\n",
              " <a list of 40 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "len_q = []\n",
        "len_a = []\n",
        "for q, a in zip(questions, answers):\n",
        "    len_q.append(len(q))\n",
        "    len_a.append(len(a))\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
        "ax[0].hist(len_q, bins=40)    \n",
        "ax[1].hist(len_a, bins=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t__hwNxWE6Be"
      },
      "source": [
        "# **Step3. Preprocessing Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUBS-g2uFJ1p"
      },
      "outputs": [],
      "source": [
        "# 전처리 함수 정의\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!가-힣ㄱ-ㅎㅏ-ㅣ0-9]+\", \" \", sentence)\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-SlwrrmGBSI"
      },
      "outputs": [],
      "source": [
        "# 데이터 토큰화\n",
        "mecab = Mecab()\n",
        "max_len = 30\n",
        "\n",
        "def build_corpus(src, tgt):\n",
        "    \n",
        "    src_cor = []\n",
        "    tgt_cor = []\n",
        "    \n",
        "    for s, t in zip(src, tgt):\n",
        "        if len(s) <= max_len:\n",
        "            src_cor.append(mecab.morphs(preprocess_sentence(s)))\n",
        "            tgt_cor.append(mecab.morphs(preprocess_sentence(t)))\n",
        "    \n",
        "    \n",
        "    return src_cor, tgt_cor\n",
        "\n",
        "que_corpus, ans_corpus = build_corpus(questions, answers) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddIo5NL1rzja"
      },
      "source": [
        "# **Step4. Data Augmentation**\n",
        "우리에게 주어진 데이터는 1만개 가량으로 적은편에 속합니다. 따라서 Lexical Substitution을 적용해 데이터를 늘려주도록하겠습니다. 아래 링크를 참고하여 한국어로 사전 훈련된 임베딩 모델을 다운로드합니다.\n",
        "https://github.com/Kyubyong/wordvectors\n",
        "\n",
        "해당 링크에서 Korean(w)를 찾아 다운로드하고, ko.bin파일을 이용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_lrGCOsu2e0"
      },
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec.load('/content/drive/MyDrive/GoingDeeper_Data/GD6/ko.bin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpSJVFVU9Yd5"
      },
      "source": [
        "불러온 모델은 아래와 같이 활용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrPkoxpM7vyo",
        "outputId": "8c4e2745-2df3-497a-a547-5291814c88f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('호랑', 0.7221653461456299),\n",
              " ('거미', 0.7095274925231934),\n",
              " ('족제비', 0.6975173950195312),\n",
              " ('잠자리', 0.6971200108528137),\n",
              " ('네발', 0.6922956109046936),\n",
              " ('개구리', 0.6853677034378052),\n",
              " ('지렁이', 0.6686853170394897),\n",
              " ('캥거루', 0.6567738056182861),\n",
              " ('고슴도치', 0.6552319526672363),\n",
              " ('해파리', 0.6538781523704529)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model.most_similar(\"나비\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3cY7cpEvlTv"
      },
      "outputs": [],
      "source": [
        "# Lexical Substitution 구현하기\n",
        "def lexical_sub(sentence, word2vec):\n",
        "    res = \"\"\n",
        "    toks = sentence\n",
        "\n",
        "    try:\n",
        "        _from = random.choice(toks)\n",
        "        _to = word2vec.most_similar(_from)[0][0]\n",
        "\n",
        "    except: # 단어장에 없는 단어\n",
        "        return None\n",
        "\n",
        "    for tok in toks:\n",
        "        if tok is _from :\n",
        "            res += _to + \" \"\n",
        "        else:\n",
        "            res += tok + \" \"\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "해당함수를 한번 사용해볼까요?"
      ],
      "metadata": {
        "id": "IS5VLU8zBAiD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "IaGvCNNO9gnu",
        "outputId": "a2e699f3-48cf-4fec-e6e0-4f8e2736a0c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sns 보 면 나 만 빼 고 으며 행복 해 보여 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "sentence = ['sns', '보', '면', '나', '만', '빼', '고', '다', '행복', '해', '보여']\n",
        "lexical_sub(sentence, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmentation된 que_corpus와 원본 ans_corpus가 병렬을 이루도록, 이후엔 반대로 원본que_corpus와 Augmentation된 ans_corpus가 병렬을 이루도록 하여 전체 데이터가 원래의 3배가량으로 늘어나도록 합니다."
      ],
      "metadata": {
        "id": "sH8CnXzVBwME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **que_corpus Augmentation**"
      ],
      "metadata": {
        "id": "GPLQc-oIB8xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_que_corpus = []\n",
        "\n",
        "for old_src in tqdm(que_corpus):\n",
        "    new_src = lexical_sub(old_src, model)\n",
        "    if new_src is not None:\n",
        "        new_que_corpus.append(new_src)\n",
        "    # Augmentation이 없더라도 원본 문장을 포함시킵니다.\n",
        "    else:\n",
        "        new_que_corpus.append(old_src)\n",
        "print()\n",
        "print('Augmentation전 Q길이: ',len(que_corpus))\n",
        "print('Augmentation후 Q길이: ',len(new_que_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4DQKfPaCG46",
        "outputId": "4c3d522f-85f7-4dba-b717-2a7a2e93b539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/11650 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n",
            "100%|██████████| 11650/11650 [00:13<00:00, 874.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Augmentation전 Q길이:  11650\n",
            "Augmentation후 Q길이:  11650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ans_corpus Augmentation**"
      ],
      "metadata": {
        "id": "Lni2ihSoEpBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_ans_corpus = []\n",
        "\n",
        "for old_src in tqdm(ans_corpus):\n",
        "    new_src = lexical_sub(old_src, model)\n",
        "    if new_src is not None:\n",
        "        new_ans_corpus.append(new_src)\n",
        "    # Augmentation이 없더라도 원본 문장을 포함시킵니다.\n",
        "    else:\n",
        "        new_ans_corpus.append(old_src)\n",
        "print()\n",
        "print('Augmentation전 A길이: ',len(ans_corpus))\n",
        "print('Augmentation후 A길이: ',len(new_ans_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxRSg99DFpQX",
        "outputId": "6ff9f075-081f-4d13-a158-bf4e639b4d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/11650 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n",
            "100%|██████████| 11650/11650 [00:13<00:00, 864.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Augmentation전 A길이:  11650\n",
            "Augmentation후 A길이:  11650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어그멘테이션한 데이터들을 전부 합쳐줍시다."
      ],
      "metadata": {
        "id": "i21-enBnF5Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Augmented_que = que_corpus + new_que_corpus + que_corpus\n",
        "print('Q길이 : ',len(Augmented_que))\n",
        "Augmented_ans = ans_corpus + ans_corpus + new_ans_corpus\n",
        "print('A길이 : ',len(Augmented_ans))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z6OzXhyF-k9",
        "outputId": "6b18104f-65f1-483e-a19c-7fb5abc3af9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q길이 :  34950\n",
            "A길이 :  34950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이로써 데이터가 거의 3배가량 늘어났습니다."
      ],
      "metadata": {
        "id": "cu9cUL-UGkcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step5. 데이터 벡터화**\n",
        "타겟 데이터인 ans_corpus에 \\<start>토큰과 \\<end>토큰이 추가되지 않은상태 이를 먼저 해결한후 벡터화를 진행합니다."
      ],
      "metadata": {
        "id": "KU99sgYjG5TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "change_ans = []\n",
        "\n",
        "for ans in Augmented_ans:\n",
        "    re_ans = [\"<start>\"] + list(ans) + [\"<end>\"]\n",
        "    change_ans.append(re_ans)\n",
        "\n",
        "change_ans[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98b_-ahNJCO0",
        "outputId": "7ae7531f-fd1a-44d7-b902-166e10af6755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<start>', '하루', '가', '또', '가', '네요', '.', '<end>'],\n",
              " ['<start>', '위로', '해', '드립니다', '.', '<end>'],\n",
              " ['<start>', '여행', '은', '언제나', '좋', '죠', '.', '<end>']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "챗봇 훈련 데이터의 가장 큰 특징 중 하나는 바로 소스데이터와 타겟데이터가 같은 언어를 사용한다는 것입니다. 이는 임베딩 층을 공유했을 때 많은 이점을 얻을 수 있습니다."
      ],
      "metadata": {
        "id": "2LOCXAKQMeq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문,답변 말뭉치 병합\n",
        "total_corpus = Augmented_que + change_ans\n",
        "# 토크나이저 생성\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "# 사전 구축\n",
        "tokenizer.fit_on_texts(total_corpus)\n",
        "# 인코더 정수시퀀스 생성\n",
        "enc_train = tokenizer.texts_to_sequences(Augmented_que)\n",
        "enc_train = pad_sequences(enc_train, padding='post')\n",
        "# 디코더 정수시퀀스 생성\n",
        "dec_train = tokenizer.texts_to_sequences(change_ans)\n",
        "dec_train = pad_sequences(dec_train, padding='post')"
      ],
      "metadata": {
        "id": "dJwB5an1O2Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 크기\n",
        "VOCAB_SIZE = len(tokenizer.index_word)\n",
        "VOCAB_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ_PzdNtQ2p2",
        "outputId": "1af0c5d0-c9af-4d1d-f71d-e7bad1e2a48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7936"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "벡터화와 패딩이 잘 적용됐는지 확인해 봅시다."
      ],
      "metadata": {
        "id": "_i6Y0xxjQ5p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zugXWonMPcFr",
        "outputId": "f2191c99-a31f-4d1a-e906-6c0cb28b05e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34950, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_train[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhBYCGtFQNIP",
        "outputId": "551fbd16-c2d8-428a-ef22-6c2fe39bf172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2531,   68, 4380,  103,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [ 318, 3648,  619, 1063,   10,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dec_train[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeWAxC_CQQpB",
        "outputId": "3a36acf1-e27e-484c-8093-d58fd2e15e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   3,  327,    9,  163,    9,   56,    2,    4,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   3,  687,   13, 1822,    2,    4,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터는 준비가 다 되었습니다. 이제 트랜스포머 모델을 설계해 줍시다."
      ],
      "metadata": {
        "id": "-ZMelmvMTTlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step6. 모델설계**\n",
        "## 1.Positional Encoding\n",
        "어순에대한 정보가 들어있는 포지셔널 행렬을 생성해주는 함수입니다."
      ],
      "metadata": {
        "id": "MXX05a3KTfZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "\n",
        "    # pos/10000^(i/d_model)생성\n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, int(i) / d_model)\n",
        "    \n",
        "    # d_model열까지 cal_angle()함수값 계산\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "    \n",
        "    # 각 행(문장)마다 get_posi_angle_vec()함수 적용해서 넘파이 배열로 저장 \n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "    # 짝수 열(2i)에 해당하는 값만 sin()함수에 넣어서 테이블에 저장\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    # 홀수 열(2i+1)에 해당하는 값만 cos()함수에 넣어서 테이블에 저장\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "\n",
        "    # 포지서널 인코딩 값이 계산된 행렬\n",
        "    return sinusoid_table"
      ],
      "metadata": {
        "id": "zapICgr_TpSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Multi-Head Attention\n",
        "어텐션을 병렬수행해주는 함수입니다."
      ],
      "metadata": {
        "id": "V58oAKqDU_CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        # Head로 쪼갠 Embedding들끼리 유사한 특성을 가진다는 보장이 없기 때문에 앞단에 Linear 레이어를 추가해 주는 겁니다.\n",
        "        # Linear 레이어는 데이터를 특정 분포로 매핑 시키는 역할을 해주기 때문에, 설령 단어들의 분포가 제각각이더라도, \n",
        "        # Linear 레이어는 Multi-Head Attention이 잘 동작할 수 있는 적합한 공간으로 Embedding을 매핑합니다.\n",
        "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        # 비슷한 이유로 각각의 Head가 Attention 한 값이 균일한 분포를 가질 거란 보장이 없습니다. \n",
        "        # 따라서 모든 Attention 값을 합쳐준 후, 최종적으로 Linear 레이어를 거치며 비로소 Multi-Head Attention이 마무리가 됩니다.\n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    # 임베딩벡터를 헤드수로 분할하는 함수\n",
        "    # x : [batch x length x emb]\n",
        "    # return : [batch x length x heads x self.depth]\n",
        "    def split_heads(self, x):\n",
        "        # 입력데이터 개수\n",
        "        batch_size = x.shape[0]\n",
        "        # 입력데이터 reshape\n",
        "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        # 입력데이터 차원의 1번과 2번인덱스 자리변경\n",
        "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    # 분할된 입력으로부터 어텐션 값을 구하는 함수\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        # K의 길이를 실수형태로 d_k에 저장\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "        \n",
        "        # Scaled QK 값 구하기\n",
        "        QK = tf.matmul(Q, K, transpose_b=True)\n",
        "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None: scaled_qk += (mask * -1e9)\n",
        "\n",
        "        # 1. Attention Weights 값 구하기 -> attentions\n",
        "        # 2. Attention 값을 V에 곱하기 -> out\n",
        "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
        "        out = tf.matmul(attentions, V)\n",
        "        return out, attentions\n",
        "\n",
        "    # 분할된 Head를 다시 하나로 결합시켜주는 함수\n",
        "    # x: [ batch x length x heads x self.depth ]\n",
        "    # return: [ batch x length x emb ]\n",
        "    def combine_heads(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
        "\n",
        "        return combined_x\n",
        "    \n",
        "    def call(self, Q, K, V, mask):\n",
        "\n",
        "        # Step1 : Linear_in(Q, K, V) -> WQ, WK, WV\n",
        "        WQ = self.W_q(Q)\n",
        "        WK = self.W_k(K)\n",
        "        WV = self.W_v(V)\n",
        "\n",
        "        # Step2 : Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
        "        WQ_splits = self.split_heads(WQ)\n",
        "        WK_splits = self.split_heads(WK)\n",
        "        WV_splits = self.split_heads(WV)\n",
        "\n",
        "        # Step3 : scaled Dot Product Attention\n",
        "        out, attention_weights = self.scaled_dot_product_attention(\n",
        "            WQ_splits, WK_splits, WV_splits, mask)\n",
        "        \n",
        "        # Step4 : combine Heads\n",
        "        out = self.combine_heads(out)\n",
        "        \n",
        "        # Step5 : Linear_out\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out, attention_weights"
      ],
      "metadata": {
        "id": "LVWg1on3Wn_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Position-wise Feed_Forward Network\n",
        "활성화함수 ReLU를 적용해주는 함수입니다."
      ],
      "metadata": {
        "id": "Ldux020Db1TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
        "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.w_1(x)\n",
        "        out = self.w_2(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZZrTllleb_4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.인코더 레이어"
      ],
      "metadata": {
        "id": "nGCKqKCedWar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        # Multi-Head Attention\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        # Position-Wise Fedd Forward Network\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, enc_attn"
      ],
      "metadata": {
        "id": "wpa65BTgdaUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.디코더 레이어"
      ],
      "metadata": {
        "id": "C3JoSfMVfJzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "        # Masked Multi-Head Attention\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        # Multi-Head Attention\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_3(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, dec_attn, dec_enc_attn"
      ],
      "metadata": {
        "id": "NG7kKpZ0fNgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.인코더"
      ],
      "metadata": {
        "id": "9HAUub4Mhwlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        # 층수 설정\n",
        "        self.n_layers = n_layers\n",
        "        # 설정된 층수를 반복문으로 인코더계층 리스트생성\n",
        "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        out = x\n",
        "        enc_attns = list()\n",
        "        \n",
        "        # 각 계층별로 어텐션 가중치(확률분포)를 리스트에 저장 \n",
        "        for i in range(self.n_layers):\n",
        "            out, enc_attn = self.enc_layers[i](out, mask)\n",
        "            enc_attns.append(enc_attn)\n",
        "        \n",
        "        return out, enc_attns"
      ],
      "metadata": {
        "id": "nSZvtNpg_BKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.디코더"
      ],
      "metadata": {
        "id": "o-vfCfPrAzkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "        out = x\n",
        "        dec_attns = list()\n",
        "        dec_enc_attns = list()\n",
        "\n",
        "        # 각 계층별 두개의 어텐션 가중치(확률분포)를 각 리스트에 저장\n",
        "        for i in range(self.n_layers):\n",
        "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
        "            dec_attns.append(dec_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "        \n",
        "        return out, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "z6VcIpfnA3xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.트랜스포머\n",
        "여기서 인코더와 디코더 각각의 임베딩과 출력층의 Linear계층, 총 3개의 레이어가 가중치를 공유합니다"
      ],
      "metadata": {
        "id": "5a0yHP1HCdek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 src_vocab_size,\n",
        "                 tgt_vocab_size,\n",
        "                 pos_len,\n",
        "                 dropout=0.2,\n",
        "                 shared_fc=True,\n",
        "                 shared_emb=False):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        \n",
        "        if shared_emb:\n",
        "            self.enc_emb = self.dec_emb = \\\n",
        "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "        else:\n",
        "            # 1.Embedding Layer 정의\n",
        "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        # 2.Positional Encoding 정의\n",
        "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "        \n",
        "        # 3.Dropout 정의\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        # 4.인코더/디코더 정의\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "        # 5.Output Linear 정의\n",
        "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "        # 6.Shared Weights\n",
        "        self.shared_fc = shared_fc\n",
        "        if shared_fc: \n",
        "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "    \n",
        "    def embedding(self, emb, x):\n",
        "        # 문장길이 저장\n",
        "        seq_len = x.shape[1]\n",
        "        \n",
        "        # 임베딩벡터 저장\n",
        "        out = emb(x)\n",
        "        \n",
        "        # 1.가중치를 공유하는 두레이어의 feature분포가 다르기때문에, sqrt(d_model)을 곱해 분포를 맞춰주는과정\n",
        "        # 2.포지셔널 인코딩이 임베딩벡터에 큰 영향을 끼치는것을 방지\n",
        "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
        "\n",
        "        # 포지셔널인코딩과 임베딩벡터의 연산을 위해 차원을 변경시켜줌\n",
        "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
        "\n",
        "        # 드롭아웃적용\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
        "\n",
        "        # 1.Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
        "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "\n",
        "        # 2.Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
        "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "\n",
        "        # 3.Decoder(dec_in, enc_out, mask) -> dec_out, dec_attns, dec_enc_attns\n",
        "        dec_out, dec_attns, dec_enc_attns = \\\n",
        "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
        "\n",
        "        # 4.Out Linear(dec_out) -> logits\n",
        "        logits = self.fc(dec_out)\n",
        "\n",
        "        return logits, enc_attns, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "KYsPceYmCgX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.마스킹"
      ],
      "metadata": {
        "id": "jEoVFHEWKjbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 마스킹 : 문장에서 패딩인 부분을 전부 찾아주는 함수\n",
        "def generate_padding_mask(seq):\n",
        "    # 문장에서 0인것을 전부 True, 아니면 False로 바꿔주고 이들을 전부 1과 0으로 표현\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    # 문장형태에서 1과 2번인덱스에 차원추가\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "# 룩 어헤드 마스킹 : 마스크를 해줄요소가 전부1이고 아니면0인 행렬 생성\n",
        "def generate_causality_mask(src_len, tgt_len):\n",
        "    # np.eye(a, b) : a행b열의 단위행렬을 만들어줌\n",
        "    # np.cumsum(a, axis=0) : 행을 그 다음행에 중첩하면서 더해줌, shape은 변하지않음\n",
        "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
        "    return tf.cast(mask, tf.float32)\n",
        "\n",
        "# 패딩 마스크와 룩 어헤드 마스크를 동시에 적용하는 함수\n",
        "def generate_masks(src, tgt):\n",
        "    # 입력과 타깃문장 패딩 마스킹 생성\n",
        "    enc_mask = generate_padding_mask(src)\n",
        "    dec_mask = generate_padding_mask(tgt)\n",
        "\n",
        "    # 인코더-디코더 어텐션 룩어헤드마스킹 생성\n",
        "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1]) \n",
        "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
        "\n",
        "    # 디코더 어텐션 룩어헤드마스킹 생성\n",
        "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
        "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
        "\n",
        "    return enc_mask, dec_enc_mask, dec_mask"
      ],
      "metadata": {
        "id": "s1lgt-nC3HhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step7. 훈련하기**\n",
        "데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. \n",
        "- Hyperparameters\n",
        "    - n_layers: 1\n",
        "    - d_model: 368\n",
        "    - n_heads: 8\n",
        "    - d_ff: 1024\n",
        "    - dropout: 0.2\n",
        "\n",
        "- Training Parameters\n",
        "    - Warmup Steps: 1000\n",
        "    - Batch Size: 64\n",
        "    - Epoch At: 10"
      ],
      "metadata": {
        "id": "Iu_9eCB5OMuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델인스턴스 생성\n",
        "transformer = Transformer(\n",
        "    n_layers=1,\n",
        "    d_model=368,\n",
        "    n_heads=8,\n",
        "    d_ff=1024,\n",
        "    src_vocab_size=20000,\n",
        "    tgt_vocab_size=20000,\n",
        "    pos_len=200,\n",
        "    dropout=0.2,\n",
        "    shared_fc=True,\n",
        "    shared_emb=True)\n",
        "\n",
        "d_model = 368"
      ],
      "metadata": {
        "id": "5zqxe29COQOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Learning Rate Schedular"
      ],
      "metadata": {
        "id": "DgWj83t1Ovdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=1000):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    # 학습률 스케줄러 수식에 해당하는 전개과정\n",
        "    # 아래 과정으로 생선된 값을 learning rate로 사용\n",
        "    def __call__(self, step):\n",
        "        arg1 = step ** -0.5\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "LrpyPz5zO0-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = LearningRateScheduler(d_model=368)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "rMmjbkJCRKLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Loss function"
      ],
      "metadata": {
        "id": "7S75LrTDRdFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logits : 텐서플로우에서 소프트맥스함수를 거치기전의 데이터\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    # 마스킹되지 않은 입력의 개수로 Scaling하는 과정\n",
        "    # 결론적으로 마스킹된부분의 loss는 계산하지않음\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    # tf.reduce_sum() : 입력행렬내에서 설정된 axis를 기준으로 sum연산이 수행됨, 차원감소   \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "pQmGOELRRhHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Train_step"
      ],
      "metadata": {
        "id": "D1X-k3MFSdTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function()\n",
        "def train_step(src, tgt, model, optimizer):\n",
        "    # 시작토큰 제외\n",
        "    gold = tgt[:, 1:]\n",
        "\n",
        "    # 마스크행렬 생성\n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
        "\n",
        "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
        "        loss = loss_function(gold, predictions[:, :-1])\n",
        "        # tape.gradient() : 매스텝 학습이 진행될때마다 발생하는 그래디언트 추출\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다.\n",
        "    # gradient가 업데이트 해야할 파라미터 지정\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "633TictGGk8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Train"
      ],
      "metadata": {
        "id": "D1WQqYIcV08L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
        "    np.random.shuffle(idx_list)\n",
        "    t = tqdm(idx_list)\n",
        "\n",
        "    for (batch, idx) in enumerate(t):\n",
        "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
        "                   dec_train[idx:idx+BATCH_SIZE],\n",
        "                   transformer,\n",
        "                   optimizer)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
        "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyhPK4ZGW-jb",
        "outputId": "7181ebb7-3655-41d4-8390-f2a8a1bfd8a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch  1: 100%|██████████| 547/547 [01:25<00:00,  6.42it/s, Loss 4.1190]\n",
            "Epoch  2: 100%|██████████| 547/547 [01:23<00:00,  6.52it/s, Loss 2.3719]\n",
            "Epoch  3: 100%|██████████| 547/547 [01:23<00:00,  6.53it/s, Loss 1.5976]\n",
            "Epoch  4: 100%|██████████| 547/547 [01:23<00:00,  6.53it/s, Loss 1.0477]\n",
            "Epoch  5: 100%|██████████| 547/547 [01:23<00:00,  6.53it/s, Loss 0.7667]\n",
            "Epoch  6: 100%|██████████| 547/547 [01:23<00:00,  6.52it/s, Loss 0.6049]\n",
            "Epoch  7: 100%|██████████| 547/547 [01:23<00:00,  6.52it/s, Loss 0.5010]\n",
            "Epoch  8: 100%|██████████| 547/547 [01:23<00:00,  6.52it/s, Loss 0.4125]\n",
            "Epoch  9: 100%|██████████| 547/547 [01:23<00:00,  6.52it/s, Loss 0.3602]\n",
            "Epoch 10: 100%|██████████| 547/547 [01:23<00:00,  6.52it/s, Loss 0.3152]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습이 완료되었습니다. 이제 해당모델을 가지고, 문장을 생성해 봅시다.\n",
        "## 5.문장생성함수"
      ],
      "metadata": {
        "id": "ONLPiEWnpUS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수시퀀스를 디코딩해주는함수\n",
        "def get_decoded_sentence(sentence):\n",
        "            return ' '.join([tokenizer.index_word[index] for index in sentence])\n",
        "\n",
        "# 입력문장을 새로운문장으로 출력\n",
        "def evaluate(sentence, model):\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    pieces = mecab.morphs(sentence)\n",
        "    tokens = tokenizer.texts_to_sequences(pieces)\n",
        "    _input = pad_sequences(tokens, maxlen=enc_train.shape[-1], padding='post')\n",
        "    \n",
        "    ids = [] \n",
        "    output = tf.expand_dims([3], 0) \n",
        "    \n",
        "    for i in range(dec_train.shape[-1]):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
        "\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
        "                \n",
        "        # 예측 단어가 종료 토큰일 경우\n",
        "        if 4 == predicted_id:\n",
        "            result = get_decoded_sentence(ids)           \n",
        "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
        "        \n",
        "        # word_to_index\n",
        "        ids.append(predicted_id)\n",
        "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
        "\n",
        "    result = get_decoded_sentence(ids)\n",
        "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
        "\n",
        "def generate(sentence, model):\n",
        "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "    evaluate(sentence, model)\n",
        "    \n",
        "    print('\\nInput: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "OOzoDUtVNgE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"지루하다, 놀러가고 싶어.\", transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3B7zEdcYssR",
        "outputId": "858a92f0-ab20-48aa-f589-79674c35e7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: 지루하다, 놀러가고 싶어.\n",
            "Predicted translation: 마음 이 어디 든 좋 을 가 보 세요 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"오늘 일찍 일어났더니 피곤하다.\", transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpN4LXXKpdfE",
        "outputId": "cf157334-04fc-4284-c38e-b0811525ae07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: 오늘 일찍 일어났더니 피곤하다.\n",
            "Predicted translation: 내일 은 더 나 을 수 있 을까요 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"간만에 여자친구랑 데이트 하기로 했어.\", transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6DjggIqpjdD",
        "outputId": "34483c2a-03c3-4dcb-eb5f-21d3b248f4ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
            "Predicted translation: 떨리 는 새 새 로 떨리 죠 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"집에 있는다는 소리야.\", transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06VV6N-OpmD9",
        "outputId": "d0012873-4ef8-4f76-ff51-6b14cc0f6bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: 집에 있는다는 소리야.\n",
            "Predicted translation: 집   에   집   에   집   하   는   집   어 요   .  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "질문과 답변에 어느정도 개연성이 보이지만, 문법이 조금 안맞네요."
      ],
      "metadata": {
        "id": "zZOu8ZOkaMJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step8.성능 측정하기**\n",
        "## 1.NLTK를 활용한 BLEU Score"
      ],
      "metadata": {
        "id": "CcYgtD11qGGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
        "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
        "\n",
        "print(\"원문:\", reference)\n",
        "print(\"번역문:\", candidate)\n",
        "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njPrKH74Sah9",
        "outputId": "5179976f-2fa5-4d54-e225-f20030ad9a78"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
            "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
            "BLEU Score: 8.190757052088229e-155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU score가 50점을 넘으면 정말 멋진 번역을 생성했다는 것입니다. 보통 목표치도 20~40점 정도이구요. 하지만 방금 나온 점수는 사실상 0점이라고 봐도 무방합니다. 왜 이렇게 낮은 점수가 나왔을까요?"
      ],
      "metadata": {
        "id": "Tvy7UitISh3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
        "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
        "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
        "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdtr5mamS6A0",
        "outputId": "a9d5b671-f88d-48e1-8f05-b78a42318c60"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-gram: 0.5\n",
            "2-gram: 0.18181818181818182\n",
            "3-gram: 2.2250738585072626e-308\n",
            "4-gram: 2.2250738585072626e-308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "바로 3-gram과 4-gram에서 거의 0점을 받았기 때문입니다. \n",
        "## 2.SmoothingFunction()으로 BLEU Score 보정하기\n",
        "그래서 BLEU계산시 특정 N-gram이 0점이 나와서 너무 작아지는 문제를 보완하기 위해 SmoothingFunction()을 사용하고 있습니다. 이는 모든 Precision에 아주 작은 epsilon값을 더해주는 역할을 합니다."
      ],
      "metadata": {
        "id": "pd-ZbFUKS99r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
        "    return sentence_bleu([reference],\n",
        "                         candidate,\n",
        "                         weights=weights,\n",
        "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
        "\n",
        "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
        "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
        "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
        "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
        "\n",
        "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9zyI92oTuW4",
        "outputId": "b2e891bb-6811-4f31-e0e0-f14dc62ba1dd"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1: 0.5\n",
            "BLEU-2: 0.18181818181818182\n",
            "BLEU-3: 0.010000000000000004\n",
            "BLEU-4: 0.011111111111111112\n",
            "\n",
            "BLEU-Total: 0.05637560315259291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SmoothingFunction()으로 BLEU score를 보정한 결과, 무려 5점으로 올라갔습니다. \n",
        "## 3. 트랜스포머 모델의 번역 성능 알아보기\n",
        "위 예시들을 이용해서 우리가 훈련한 모델이 얼마나 번역을 잘하는지 평가할 수 있습니다.\n",
        "\n",
        "한 문장의 BLEU를 계산하는 calculate_bleu()을 만들어봅시다."
      ],
      "metadata": {
        "id": "RPNdyQezTyYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
        "    # 타겟문장\n",
        "    reference = tgt_sentence.split()\n",
        "    # 예측문장 생성\n",
        "    pieces, results, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "    evaluate(src_sentence, model)\n",
        "    candidate = results.split()\n",
        "    # bleu스코어 계산\n",
        "    score = sentence_bleu([reference], candidate,\n",
        "                          smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Source Sentence: \", src_sentence)\n",
        "        print(\"Model Prediction: \", candidate)\n",
        "        print(\"Real: \", reference)\n",
        "        print(\"Score: %lf\\n\" % score)\n",
        "        \n",
        "    return score"
      ],
      "metadata": {
        "id": "n0rqv8xQUwyb"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "해당 함수를 토대로 훈련데이터에있는 문장으로 BLEU스코어를 계산해 봅시다."
      ],
      "metadata": {
        "id": "11ZgQDWdZ17E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_bleu(transformer, \n",
        "                 \"3박4일 정도 놀러가고 싶다\", \n",
        "                 \"여행은 언제나 좋죠.\", \n",
        "                 mecab, \n",
        "                 mecab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyXVnb8ZWfRE",
        "outputId": "659aeb42-1339-4e51-cfc9-5691dd8dc7c1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sentence:  3박4일 정도 놀러가고 싶다\n",
            "Model Prediction:  ['잘', '해결', '되', '길', '바라', '요', '.']\n",
            "Real:  ['여행은', '언제나', '좋죠.']\n",
            "Score: 0.000000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_bleu(transformer, \n",
        "                 \"1지망 학교 떨어졌어\", \n",
        "                 \"위로해 드립니다.\", \n",
        "                 mecab, \n",
        "                 mecab)\t\t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzYfkv53ZldL",
        "outputId": "c9ed035a-e02e-422b-9f81-21100adfdf0c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sentence:  1지망 학교 떨어졌어\n",
            "Model Prediction:  ['잘', '되', '길', '바라', '요', '.']\n",
            "Real:  ['위로해', '드립니다.']\n",
            "Score: 0.000000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_bleu(transformer, \n",
        "                 \"SNS보면 나만 빼고 다 행복해보여\", \n",
        "                 \"자랑하는 자리니까요.\", \n",
        "                 mecab, \n",
        "                 mecab)\t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWoyAB8qaDmS",
        "outputId": "b7dd7009-1911-4005-d2f1-17f3bef80dbc"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sentence:  SNS보면 나만 빼고 다 행복해보여\n",
            "Model Prediction:  ['sns', '흔적', '삭제', '하', '세요', '.']\n",
            "Real:  ['자랑하는', '자리니까요.']\n",
            "Score: 0.000000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "두번째문장은 괜찮은 답변이긴하지만, 실제답변과 일치하지않아 0점이 나오네요.\n",
        "# **회고**\n",
        "- Data Augmentation을 진행할때 한국어로 사전 훈련된 임베딩 모델을 사용했는데, 딱히 맘에 들진 않았다. '나비'와 높은 유사도 순위가 '호랑, '거미', '족제비'라니...\n",
        "- 그리고 Lexical Substitution을 진행할때 문장의 한개의 토큰만 대체를 했는데 한개가 아닌 2개,3개정도 어휘대체를하면 더 좋은 결과를 낼수 있을꺼같다.\n",
        "- 처음 에포크를 돌렸을때 계속 오류가났었는데 입력데이터에 문제가있었다. 길이를 출력해보니까 인코더와 디코더입력데이터가 달랐다. 알고보니까, 인코더 인풋데이터를 어그멘테이션하기 전의 값을 사용한것이었다...그러니 당연히 shape이 맞지않으니 안돌아가는것이었다.\n",
        "- 가장 많은 시간을 할애한 오류는 VOCAB_SIZE인거 같다..! 첫에포크를 돌릴때 계속해서 loss가 nan이 나왔다. 구글링해보았지만, 원하는 답변은 딱히 없었다. 결론은 트랜스포머 인스턴스를 생성할때 src_vocab_size와 tgt_vocab_size값이 너무 작아서였다. 모델 설계를 보면 해당 파라미터는 임베디 벡터의 행개수를 뜻한다. 내가 알고있는 이론으로는 임베딩벡터의 행은 단어사전의 수, 열은 임베딩차원(d_model, 임의값 설정)로 알고있었다. 그래서 단어사전수인 약7900정도의 값을 넣고 에폭을 돌리면 loss가 nan이 나온다. 그래서 이를 20000정도로 늘려서 해주니 정상적으로 돌아갔다...왜인지 도무지 이해가 안된다. 옆자리인 명님 말로는 단어사전크기를 7900으로하고 warm step을 낮추면 돌아간다고한다. 하지만 나도 warm_step을 낮춰봤지만 소용이 없었다. 뭐지..?\n",
        "- BLEU스코어가 정말 챗봇의 성능을 측정할 수 있는지가 의문이다. 몇 문장은 질문에 대한 답변이 꽤 개연성있게 나왔다고 생각한다. 하지만 이를 실제 타겟문장과 비교해 BLEU score를 계산하니 당연히 0이 나왔다. 질문에 대한 답변이 정해져있다는것부터 별로인거같다."
      ],
      "metadata": {
        "id": "Vz85iyywZ7XP"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}